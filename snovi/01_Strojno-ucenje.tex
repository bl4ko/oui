\section{Strojno ucenje}


\subsection{Problemski prostor, ocenjevanje znanja}

\subsection{Evalviranje hipotez}
Pomembni kriteriji:
\begin{itemize}[leftmargin=*,labelindent=0pt,labelwidth=0pt,itemsep=0pt,parsep=0pt,topsep=0pt]
    \item \textbf{konsistentnost} hipotez z primeri (ucnimi)
    \item \textbf{splosnost} (tocnost za nevidene primere)
    \item \textbf{razumljivost} hipotez
\end{itemize}
\green{TP}=true positive, \green{TN}-true negative, \green{FP}-false positive (\blue{napaka 1. tipa}), \green{FN}-false negative (\blue{napaka 2. tipa}) \\
$\text{\green{Klasifikacijska tocnost}}=\frac{TP+TN}{TP+TN+FP+FN}=\frac{TP+TN}{N}$\\
$\text{\green{Obcutljivost/senzitivnost}}=TPR=\frac{TP}{TP+FN}$

\subsection{Gradnja odlocitvenih dreves}
Za koliko se entropija zmanjsa po delitvi z Atributom A:\\
\textbf{Informacijski prispevek (najbolj informativni atribut maksimizira informacijski prispevek minimizira} $I_{res}$:\\
$\text{\green{Gain(A)}}=H(A)-H_{res}(A)$\\
$\green{H_{\text{res}}(A)}=-\sum\limits_{a_i\in A}p(A=a_i)\sum\limits_{c_i\in C} p(C=c_i|A=a_i)\log_2p(C=c_i|A=a_i)$\\
\textbf{Razmerje inofrmacijskega prispevka atributa A}:\\ 
$\text{\green{IGR(A)}}=\frac{\text{Gain(A)}}{\text{H(A)}}$


\subsubsection{TDIDT (Top down induction decision tree) algoritem}
Pozresen algoritem, ki \textbf{lokalno} izbira najbolsi atribut.\\
- kratkoviden algoritem

\subsubsection{Binarizacija atributov}
Aleternativa za resevanje problematike z vecvrednostnimi atributi:\\
\blue{Strategije} (za primer B = \{Y, G, R, B\}):
\begin{itemize}[leftmargin=*,topsep=0pt,noitemsep]
    \item $\left[\{Y \}, \{R, G, B \} \right]$ (one-vs-all)
    \item $\left[\{Y, R\}, \{G, B\}\right]$ 
    \item vpeljava bianrnih atributov za vsako barvo
\end{itemize}
Primer B = \{Y, G, R\}, konstruiramo 3 nove binarne atribute:\\
$ \begin{matrix} barva \\ Y \\ G \\ R \end{matrix} \rightarrow \begin{tabular}{c|c|c} Y & G & R \\ \hline 1 & 0 & 0 \\ 0 & 1 & 0 \\ 0 & 0 & 1\end{tabular}$
\textbf{Prednost}: manjse vejanje drevesa.


\subsection{Ucenje iz sumnih podatkov (rezanje)}
\green{tocnost t}...verjetnost pravilnosti klasifikacije\\
\green{napaka e} $\dots$ $1-t$\\
\green{relativna frekvenca} $p=\frac{n}{N}$\\
\green{m-ocena} $p=\frac{n + p_a * m}{N+m}$\\
$m \dots$ koliko zaupam apriorni verjetnosti\\
$p_a$ apriorna verjetnost (pove domenski ekspert) (v nasem primeru relativna frekvenca)\\
\green{Laplacova ocena verjetnosti} $p=\frac{n+1}{N+k}$\\
k...stevilo vseh moznih razredov

\subsubsection{MEP (Minimal Error Prunning)}
e...staticna napaka,E...vzvratna napaka,$\green{e\leq E} \rightarrow$ rezemo poddrevo\\
\includegraphics[width=6cm]{./images/mep.png}\\
(Laplace)\\
$e_L(d)=1-t=1-\frac{13+1}{20+2}=0.363$\\
$E_L(d)=12/20\cdot e_L(d_l) + 8/20\cdot e_L(d_d)= \frac{12}{20} \cdot(1-\frac{7+1}{12+2})+\frac{8}{20}(1-\frac{13+1}{20+2})$\\

\subsubsection{REP (reduced error prunning)} 
Ucna mnozica: 70\% za gradnjo, 30\% za rezanje (z rezanjem odstranimo poddrevesa, ki niso kriticna in so redundantna tako zmansamo velikost drevesa)\\
G(v)=st. napacnih klasifikacij v poddrevesu - st. napacnih klasifikacij v korenu poddrevesa\\
$G(v)\geq 0 \Rightarrow$ rezemo podrevo\\
\includegraphics[width=6.5cm]{./images/rep.png}\\
$e(C)=3$,\;\;\; $e_T=2+3=5$,\;\;\; $G(C)=5-3=2\geq0 \rightarrow \text{rezemo}$

\subsection{Ocenjevanje uspesnosti modelov}
\green{tocnost t} $\dots$ verjetnost pravilnosti klasifikacije\\
\green{Laplacova ocena verjetnosti} $p=\frac{n+1}{N+k}$\\
k...stevilo vseh moznih razredov\\
\includegraphics[width=6cm]{./images/drevo-laplace.png}\\
$t_{L1}=\frac{2+1}{3+3}=0.5$,
$t_{L2}=\frac{4+1}{7+3}=0.5$,
$t_{L3}=\frac{2+1}{2+3}=0.6$\\
tocnost drevesa: $t_D=3/12\cdot 0.5 + 7/12\cdot 0.5 + 2/12\cdot 0.6=0.5167$\\
\includegraphics[width=2cm]{./images/klasifikacijska-napaka.png}\\
$e=1-(P(B=0)P(R=0|B=0)+P(B=1)P(R=1|B=1))$

\subsection{Obravnanva mankajocih atributov, navini Bayesov klasifikator}
\subsubsection{Naivni bayes}
Ce poznamo razred, kam klasificiramo ce nepoznamo atributov:\\
\textbf{Klasifikator}: \text{argmax}$_{c\in C} P(c)\prod\limits_{i=1}^n P(x_i|c)$\\
$c\dots \text{razred}$, $x_i\dots \text{atributi}$\\
Verjetnost::\\
$P(C=c|x_1,\dots,x_n)=\frac{P(C=c)P(X_1=x_i|C=c)P(X_2=x_j|C=c)\dots}{P(X_1=x_i)P(X_2=x_j)\dots}$\\


Primer pritisk, vreme, razred glavobol
$$
\begin{array}{|c|c|c|}
    \hline
    X\backslash Y   & \text{ne}      & \text{da}\\
    \hline
    P(G)           & P(G=da) = \frac{5}{9}    & P(G=ne) = \frac{4}{9}\\
    \hline
    \text{P(P=srednji)}   & P(P=sre.|G=ne)=\frac{1}{5} & P(P=sre.|G=da)=\frac{2}{4}\\
    \hline
    \text{P(V=dezevno)} & P(V=dez|G=ne)=\frac{1}{5} & P(V=dez|G=da)=\frac{2}{4}\\
    \hline
    P(y)\prod\limits_{i=1}^n P(x_i|y) & \frac{5}{9}\cdot\frac{1}{5}\cdot\frac{1}{5} & \frac{4}{9}\cdot\frac{2}{4}\cdot\frac{2}{4}\\
    \hline
\end{array}
$$

\subsubsection{Nomogragmi}
Ciljni razred $C=c_T$\\
$X_{X_i=x_j}=\ln \left( \frac{P(X_i=x_j|C=c_T)}{P(X_i=x_j|C=\overline{c_T})} \right)$

\subsection{K-najblizjih sosedov}
\includegraphics[width=6cm]{./images/knn.png}

\subsection{Nadzorovano ucenje (supervised learning)}
\textbf{Ucni primeri so podani/oznaceni} kot \textbf{vrednosti vhodov in izhodov}.\\
$(\vec{x}_1,\vec{y}_1),(\vec{x}_2,\vec{y}_2),\dots,(\vec{x}_N,\vec{y}_N)$\\
$\vec{x}_i\dots$ atributi, $\vec{y}_i\dots$ ciljna spremenljivka\\
Locimo dve vrsti problemov:
% Create the list of problems with no margins
\begin{enumerate}[leftmargin=*,noitemsep,topsep=0pt,partopsep=0pt]
    \item \textbf{Klasifikacijski problemi} - $y_j$ \underline{diskretna} 
    \item \textbf{Regresijski problemi} - $y_j$ \underline{zvezna} 
\end{enumerate}

\subsubsection{Lokalno utezena regresija}
$h(\vec{x}_?) = \frac{\sum\limits_{i=1}^k w_i\cdot f(\vec{x}_i)}{\sum\limits_{i=1}^k w_i}, \: w_i(d) ... \text{utez}$\\
\includegraphics[width=7cm]{./images/lokalno-utezena-regresija.png}

\subsubsection{Regresijska drevesa}
Linearna regresija je poseben primer regresijskega drevesa.\\
V listih regresijskega drevesa vcasih napovemo kar povprecno vrednost.

\subsection{Nenadzorovano ucenje (unsupervised learning)}
\textbf{Ucni primeri niso oznaceni} (nimajo ciljne spremenljivke),
\textbf{ucimo se vzorcev v podatkih}, (npr. grucenje)

\subsubsection{Hierarhicno grucenje}
Poveze po podobnosti med primeri, primer zacne kot samostojna gruca, na koncu vsi primeri pripadajo eni gruci\\
\textbf{Dendrogram}: drevo, ki predstavlja grucenje.\\
\textbf{Single-linkage}: povezava med grucami je najkrajsa razdalje med primeroma iz razlicnih gruc.\\
\textbf{Complete-linkage}: najdaljsa razdalja med primeroma iz razlicnih gruc.\\
\textbf{Average-linkage}: povprecna razdalja med primeroma iz razlicnih gruc.\\
Tocke A(3,1),B(1,2),C(3,4),D(5,2),E(1,1), manhattan, complete linkage:
\begin{tabular}{c|ccccc}
    & A & B & C & D & E\\
    \hline
    A & 0 & 3 & 3 & 3 & 2\\
    B &   & 0 & 4 & 4 & \underline{1}\\
    C &   &   & 0 & 4 & 5\\
    D &   &   &   & 0 & 5\\
    E &   &   &   &   & 0\\
\end{tabular}
$\rightarrow$
\begin{tabular}{c|cccc}
    & A & BE & C & D\\
    \hline
    A & 0 & \underline{3} & 3 & 3\\
    BE &   & 0 & 5 & 5\\
    C &  &   & 0 & 4\\
    D &   &   &   & 0\\
\end{tabular}
$\rightarrow$
\begin{tabular}{c|ccc}
    & ABE & C & D\\
    \hline
    ABE & 0 & 5 & 5\\
    C &   & 0 & \underline{4}\\
    D &   &   & 0\\
\end{tabular}
$\rightarrow$
\begin{tabular}{c|cc}
    & ABE & CD\\
    \hline
    ABE & 0 & 5\\
    CD &   & 0\\
\end{tabular}


    

\subsubsection{K-means}
1. V prostor dodamo k centroidov, ki predstavljajo gruce.\\
2. Izracunamo ketri centroid je najblizji vsakemu primeru.\\
3. Izracunamo nove centre gruc = $\frac{1}{|G|}\sum\limits_{i\in G} x_i$\\
4. Ponovimo korake 2 in 3 dokler se centri ne premaknejo.\\
V mnozici tock A(3,1),B(1,2),C(3,4),D(5,2),E(1), manhattanska razdalja, zacetni vrednosti centroidov C1(4,4) in C2(5,4).\\
\begin{tabular}{c|c|c|c}
    Tocka & d(X,C1) & d(X,C2) & Gruca\\
    \hline
    A & 4 & 5 & C1\\ 
    B & 5 & 6 & C1\\
    C & 1 & 2 & C1\\
    D & 3 & 2 & C2\\
    E & 6 & 7 & C1\\
    \hline
\end{tabular}\\
V naslednji iteraciji sta koordinati centroidov:\\
$C1=(\frac{3+1+3+1}{4},\frac{1+2+4+1}{4})=(2,2)$ \; in \; $C2=D=(5,2)$ ...

\subsection{Spodbujevalno ucenje - reinforcement learning}
Inteligentni agent se uci iz zaporedja \textbf{nagrad} in \textbf{kazni}

\subsection{Ocenjevanje ucenja}
\textbf{k-fold}, celo ucno mnozico razbij na k disjunktnih podmnozic za vsako od k podmnozic uporabi mnozico kot testno mnozico, preostalih k-1 mnozic kot ucno mnozico.